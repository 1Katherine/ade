# Fixed configuration
# Spark home
hibench.spark.home      /usr/local/home/spark/spark
# hibench.spark.master     yarn-client
hibench.spark.master    k8s://https://192.168.0.70:6443
spark.kubernetes.container.image     192.168.0.40/library/spark:v2.4.4

 spark.driver.memory 4g
 spark.driver.cores 1
 spark.io.compression.codec zstd
 spark.io.compression.zstd.blockSize 32k
 spark.network.timeout 120s
 spark.speculation false
 spark.serializer org.apache.spark.serializer.KryoSerializer


spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://192.168.0.70:9000/spark/log
spark.history.fs.logDirectory 	 hdfs://192.168.0.70:9000/spark/log
spark.eventLog.compress true

#======================================================
# Spark Streaming
#======================================================
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          100

# Number of nodes that will receive kafka input (default: 4)
hibench.streambench.spark.receiverNumber        4

# Indicate RDD storage level. (default: 2)
# 0 = StorageLevel.MEMORY_ONLY
# 1 = StorageLevel.MEMORY_AND_DISK_SER
# other = StorageLevel.MEMORY_AND_DISK_SER_2
hibench.streambench.spark.storageLevel 2

# indicate whether to test the write ahead log new feature (default: false)
hibench.streambench.spark.enableWAL false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
hibench.streambench.spark.checkpointPath /var/tmp

# whether to use direct approach or not (dafault: true)
hibench.streambench.spark.useDirectMode true


# Random configuration
 spark.reducer.maxReqsInFlight	6
 spark.kryoserializer.buffer	741k
 spark.reducer.maxBlocksInFlightPerAddress	32
 spark.executor.instances	6
 spark.memory.offHeap.size	1m
 spark.shuffle.file.buffer	559k
 spark.executor.memoryOverhead	1266941713m
 spark.locality.wait	1435374736s
 spark.memory.fraction	28.54
# Fixed configuration
# Spark home
hibench.spark.home      /usr/local/home/spark/spark
# hibench.spark.master     yarn-client
hibench.spark.master    k8s://https://192.168.0.70:6443
spark.kubernetes.container.image     192.168.0.40/library/spark:v2.4.4

 spark.driver.memory 4g
 spark.driver.cores 1
 spark.io.compression.codec zstd
 spark.io.compression.zstd.blockSize 32k
 spark.network.timeout 120s
 spark.speculation false
 spark.serializer org.apache.spark.serializer.KryoSerializer


spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://192.168.0.70:9000/spark/log
spark.history.fs.logDirectory 	 hdfs://192.168.0.70:9000/spark/log
spark.eventLog.compress true

#======================================================
# Spark Streaming
#======================================================
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          100

# Number of nodes that will receive kafka input (default: 4)
hibench.streambench.spark.receiverNumber        4

# Indicate RDD storage level. (default: 2)
# 0 = StorageLevel.MEMORY_ONLY
# 1 = StorageLevel.MEMORY_AND_DISK_SER
# other = StorageLevel.MEMORY_AND_DISK_SER_2
hibench.streambench.spark.storageLevel 2

# indicate whether to test the write ahead log new feature (default: false)
hibench.streambench.spark.enableWAL false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
hibench.streambench.spark.checkpointPath /var/tmp

# whether to use direct approach or not (dafault: true)
hibench.streambench.spark.useDirectMode true


# Random configuration
 spark.executor.instances	5
 spark.executor.memoryOverhead	786m
 spark.kryoserializer.buffer	47k
 spark.locality.wait	8s
 spark.memory.fraction	0.86
 spark.memory.offHeap.size	1001m
 spark.reducer.maxBlocksInFlightPerAddress	1823902617
 spark.reducer.maxReqsInFlight	1325658629
 spark.shuffle.file.buffer	45k
# Fixed configuration
# Spark home
hibench.spark.home      /usr/local/home/spark/spark
# hibench.spark.master     yarn-client
hibench.spark.master    k8s://https://192.168.0.70:6443
spark.kubernetes.container.image     192.168.0.40/library/spark:v2.4.4

 spark.driver.memory 4g
 spark.driver.cores 1
 spark.io.compression.codec zstd
 spark.io.compression.zstd.blockSize 32k
 spark.network.timeout 120s
 spark.speculation false
 spark.serializer org.apache.spark.serializer.KryoSerializer


spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://192.168.0.70:9000/spark/log
spark.history.fs.logDirectory 	 hdfs://192.168.0.70:9000/spark/log
spark.eventLog.compress true

#======================================================
# Spark Streaming
#======================================================
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          100

# Number of nodes that will receive kafka input (default: 4)
hibench.streambench.spark.receiverNumber        4

# Indicate RDD storage level. (default: 2)
# 0 = StorageLevel.MEMORY_ONLY
# 1 = StorageLevel.MEMORY_AND_DISK_SER
# other = StorageLevel.MEMORY_AND_DISK_SER_2
hibench.streambench.spark.storageLevel 2

# indicate whether to test the write ahead log new feature (default: false)
hibench.streambench.spark.enableWAL false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
hibench.streambench.spark.checkpointPath /var/tmp

# whether to use direct approach or not (dafault: true)
hibench.streambench.spark.useDirectMode true


# Random configuration
 spark.executor.instances	6
 spark.executor.memoryOverhead	737m
 spark.kryoserializer.buffer	64k
 spark.locality.wait	6s
 spark.memory.fraction	0.75
 spark.memory.offHeap.size	808m
 spark.reducer.maxBlocksInFlightPerAddress	1792715968
 spark.reducer.maxReqsInFlight	1653836966
 spark.shuffle.file.buffer	46k
